{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-16 16:04:05.715697: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-16 16:04:05.715729: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Some weights of the model checkpoint at nghuyong/ernie-1.0 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sentence_transformers import models\n",
    "from torch import nn\n",
    "\n",
    "word_embedding_model = models.Transformer('nghuyong/ernie-1.0')\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "# dense_model = models.Dense(\n",
    "#     in_features=pooling_model.get_sentence_embedding_dimension(), \n",
    "#     out_features=512,\n",
    "#     activation_function=nn.Tanh(),\n",
    "#     bias=True)\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "# model = SentenceTransformer('nghuyong/ernie-1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "wiki_info = json.load(open('data/cleaned_wiki.json'))\n",
    "train_csv = pd.read_csv('data/train.csv')\n",
    "test_csv = pd.read_csv('data/val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, wiki_info): # use riddle and the explanation of each choice to compute the similarity\n",
    "    questions = []\n",
    "    contexts = []\n",
    "    choices = []\n",
    "    labels = []\n",
    "    for idx, row in data.iterrows():\n",
    "        questions.append(f'{row[\"riddle\"]}')\n",
    "        labels.append(int(row['label']))\n",
    "        context = []\n",
    "        choice = []\n",
    "        for i in range(5):\n",
    "            name = f'choice{i}'\n",
    "            explanation = wiki_info.get(row[name], '')\n",
    "            choice.append(row[name])\n",
    "            context.append(explanation)\n",
    "        contexts.append(context)\n",
    "        choices.append(choice)\n",
    "    return questions, contexts, choices, labels\n",
    "\n",
    "questions, contexts, choices, labels = preprocess(train_csv, wiki_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "from sentence_transformers import InputExample, losses, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_examples = []\n",
    "valid_examples = []\n",
    "train_size = int(len(questions) * 0.9)\n",
    "eval_size = len(questions) - train_size\n",
    "\n",
    "# train dataset\n",
    "for i in range(train_size):\n",
    "   question, context, label = questions[i], contexts[i], labels[i]\n",
    "   for idx, text in enumerate(context):\n",
    "      if(label == idx):\n",
    "         train_examples.append(InputExample(texts=[question, text], label=1))\n",
    "         # increase ratio of positive example\n",
    "         # train_examples.append(InputExample(texts=[question, text], label=1))\n",
    "         # train_examples.append(InputExample(texts=[question, text], label=1))\n",
    "         # train_examples.append(InputExample(texts=[question, text], label=1))\n",
    "      else:\n",
    "         train_examples.append(InputExample(texts=[question, text], label=0))\n",
    "\n",
    "# valid dataset\n",
    "sentences1 = []\n",
    "sentences2 = []\n",
    "scores = []\n",
    "for i in range(train_size, len(questions)):\n",
    "   question, context, label = questions[i], contexts[i], labels[i]\n",
    "   for idx, text in enumerate(context):\n",
    "      sentences1.append(question)\n",
    "      sentences2.append(text)\n",
    "      scores.append(label == idx)\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n",
    "train_loss = losses.ContrastiveLoss(model, margin=0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820ed5a74750474286223faff15ca986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0bfd72f24b946fb8a90f31a855a587e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(train_objectives=[(train_dataloader, train_loss)], \n",
    "            epochs=1, \n",
    "            warmup_steps=30, \n",
    "            optimizer_params={'lr': 6e-5},\n",
    "            evaluator=evaluator, \n",
    "            evaluation_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, contexts, choices, labels = preprocess(test_csv, wiki_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def charnum_to_num(charnum):\n",
    "    lens={\n",
    "            '一字':1,\n",
    "            '二字':2,\n",
    "            '三字':3,\n",
    "            '四字':4,\n",
    "            '五字':5,\n",
    "            '六字':6,\n",
    "            '七字':7,\n",
    "            '八字':8,\n",
    "            '九字':9,\n",
    "            '十字':10,\n",
    "            }\n",
    "    return lens.get(charnum,None)\n",
    "def pre_select(quiz,options=None): #str,str[5]，谜面和选项，返回bool[5],bool=true代表选项筛选后可能对\n",
    "    poss=[True,True,True,True,True]#谜底都可能正确\n",
    "    #按字数筛选\n",
    "    charnum=re.findall('（.*?([一二三四五六七八九]字).*?）',quiz)#返回一个list，因为在括号内，所以不包含谜面的x字，只包含谜底字数\n",
    "    if(len(charnum)!=0):#有关于谜底字数的描述\n",
    "        num=charnum_to_num(charnum[0]) #谜底长度 注意不包括标点符号\n",
    "        for i in range(5):\n",
    "            #将options的标点都去掉，不占字数\n",
    "            tmp_option=options[i].replace('，','')\n",
    "            if(len(tmp_option)!=num):\n",
    "                poss[i]=False\n",
    "    return poss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20: tp=14\n",
      "40: tp=27\n",
      "60: tp=38\n",
      "80: tp=52\n",
      "100: tp=69\n",
      "120: tp=82\n",
      "140: tp=94\n",
      "160: tp=106\n",
      "180: tp=116\n",
      "200: tp=130\n",
      "220: tp=139\n",
      "240: tp=149\n",
      "260: tp=162\n",
      "280: tp=175\n",
      "300: tp=187\n",
      "320: tp=197\n",
      "340: tp=210\n",
      "360: tp=222\n",
      "380: tp=232\n",
      "400: tp=240\n",
      "420: tp=255\n",
      "440: tp=266\n",
      "460: tp=277\n",
      "480: tp=291\n",
      "500: tp=301\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "tot = 0\n",
    "tp = 0\n",
    "for question, context, label in zip(questions, contexts, labels):\n",
    "    pred = 0\n",
    "    val = 0\n",
    "    poss = pre_select(question, choices[tot])\n",
    "    for idx, text in enumerate(context):\n",
    "        if poss[idx] == False:\n",
    "            continue\n",
    "        embeddings1 = model.encode(question, convert_to_tensor=True)\n",
    "        embeddings2 = model.encode(text, convert_to_tensor=True)\n",
    "        score = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "        if score > val:\n",
    "            pred = idx\n",
    "            val = score\n",
    "    tp += (pred == label)\n",
    "    tot += 1\n",
    "    if tot % 20 == 0:\n",
    "        print(f'{tot}: tp={tp}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(301, 0.602)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp, tp / len(questions)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
